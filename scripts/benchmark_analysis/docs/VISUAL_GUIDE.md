# Copilot - Pending review
# Visual Output Guide

This document describes all the charts and visualizations generated by the benchmark analysis tools.

## Generated Visualizations

When you run the report command:
```bash
python cli.py report file.csv -o output_dir/
```

You get 7 PNG images (300 DPI, publication quality) + statistical summaries.

### 1. Time Distribution Histogram (`time_distribution.png`)

**What it shows**: How execution times are distributed across all successful runs.

**Chart type**: Histogram with frequency bars

**What to look for**:
- **Shape**: Normal bell curve vs skewed vs multi-modal
- **Clustering**: Tight cluster (good) vs spread out (variable)
- **Peak location**: Where most runs land
- **Tail**: Long right tail = occasional slow runs

**Interpretation**:
```
Tight peak at 450ms, small tail → Consistent performance
Spread across 400-2000ms → Unpredictable performance
Two peaks (bimodal) → Two different execution paths
```

**Stats overlay**: Shows mean, median, std dev in corner box

---

### 2. Time Distribution (Log Scale) (`time_distribution_log.png`)

**What it shows**: Same as above but with logarithmic x-axis.

**Why log scale**: Better shows outliers and heavy tails that get squished in linear view.

**When to use**: When you have extreme outliers (e.g., 500ms typical, but occasional 60000ms timeouts)

**What to look for**:
- Outliers become visible
- Can see distribution across multiple orders of magnitude
- Easier to spot bimodal patterns with one fast mode, one slow mode

---

### 3. Status Breakdown Pie Chart (`status_breakdown.png`)

**What it shows**: Percentage of SUCCESS vs TIMEOUT vs FAILED vs INFEASIBLE runs.

**Chart type**: Pie chart with percentages

**Colors**:
- Green: SUCCESS
- Red: TIMEOUT
- Orange: FAILED
- Gray: INFEASIBLE

**Interpretation**:
```
100% green → All runs successful
99.5% green, 0.5% red → Rare timeouts (acceptable)
95% green, 5% red → Frequent timeouts (problem!)
Any orange/gray → Investigate those cases
```

---

### 4. Time Series Plot (`time_series.png`)

**What it shows**: Execution time across iterations (run order).

**Chart elements**:
- Light blue dots: Individual runs
- Red line: Rolling average (smooths noise)
- Red X marks: Timeout locations

**What to look for**:
- **Trends**: Performance degrading over time? System warming up?
- **Clustering**: Timeouts clustered together or random?
- **Stability**: Rolling average flat (stable) or wandering (unstable)
- **Outlier patterns**: Spikes correlated with specific iteration ranges?

**Interpretation**:
```
Flat rolling average → Consistent across all iterations
Upward trend → Performance degrading (memory leak? cache pollution?)
Timeouts clustered → Not random, likely specific hard cases
Random scatter, flat average → Good, random test is working
```

---

### 5. Scatter Plot with Outliers (`scatter_outliers.png`)

**What it shows**: Every run as a point, with outliers highlighted.

**Chart elements**:
- Blue dots: Normal runs
- Red dots: Statistical outliers (IQR method)
- Orange X: Timeouts (shown above the plot)

**What to look for**:
- **Outlier count**: Few (normal) vs many (problem)
- **Outlier distribution**: Random or clustered iterations?
- **Gap between normal and outliers**: Gradual or sudden jump?

**Interpretation**:
```
Few red dots scattered → Normal variation
Many red dots clustered → Specific hard inputs
Huge gap between blue and red → Bimodal (two regimes)
```

---

### 6. Box Plot Comparison (`box_comparison.png`)

**What it shows**: Statistical distribution across multiple benchmark files (e.g., 10×10, 20×20, 30×30).

**Chart elements**:
- Box: Middle 50% of data (P25 to P75) - this is the IQR
- Line in box: Median (P50)
- Whiskers: Extend to 1.5×IQR beyond box
- Dots outside whiskers: Outliers

**What to look for**:
- **Box size**: Small IQR = consistent, Large IQR = variable
- **Box position**: Median shifting up with size = scaling
- **Whisker length**: Short = tight distribution, Long = spread out
- **Outlier count**: Few dots = good, many dots = many outliers

**Interpretation**:
```
10×10: Tiny box, median at 450ms → Very consistent
20×20: Bigger box, median at 600ms → More variation
30×30: Huge box, median at 680ms → Highly variable

→ Conclusion: Variability increases with problem size
```

**Box plot anatomy**:
```
    ○  ← outlier (very slow run)
    |
    ┬  ← whisker end (P75 + 1.5×IQR)
    |
    ┌─┐
    │ │ ← box (middle 50%, IQR)
    ├─┤ ← median line
    │ │
    └─┘
    |
    ┴  ← whisker end (P25 - 1.5×IQR)
```

---

### 7. Percentile Comparison (`percentile_comparison.png`)

**What it shows**: How specific percentiles (P50, P75, P90, P95, P99) change across sizes.

**Chart type**: Grouped bar chart

**What to look for**:
- **P50 scaling**: How median grows (linear, quadratic, exponential?)
- **Gap between P50 and P99**: Growing gap = worse tail latency at scale
- **P99 behavior**: If P99 explodes, you have serious tail problems

**Interpretation**:
```
Size 10: P50=450ms, P99=600ms (gap: 33%)
Size 20: P50=600ms, P99=800ms (gap: 33%)
Size 30: P50=680ms, P99=10000ms (gap: 1370%!)

→ Conclusion: Tail latency explosion at 30×30
```

---

## Additional Files Generated

### `statistics.txt`

Plain text summary with all numerical statistics. Good for:
- Quick reference
- Sharing via email/chat
- Pasting into documentation

### `statistics.json`

Machine-readable JSON with all stats. Good for:
- Loading into other tools
- Historical tracking
- Automated analysis pipelines

### `timeouts.csv` (if any timeouts exist)

CSV file with just the timeout cases. Columns:
- iteration: Which run timed out
- time_ms: How long before timeout
- spec: Full problem specification (JSON)

Good for investigating what makes specific problems hard.

---

## Quick Visual Interpretation Guide

### Healthy Benchmark Profile

Looking at the charts, you want to see:

1. **Histogram**: Tight bell curve, small right tail
2. **Log histogram**: Clean single peak, minimal scatter at extremes
3. **Pie chart**: >99% green
4. **Time series**: Flat rolling average, scattered dots
5. **Scatter**: Dense cloud of blue, few red outliers
6. **Box plot**: Small boxes, short whiskers
7. **Percentile**: P99 < 2× P50

### Problematic Profile

Red flags in the charts:

1. **Histogram**: Flat/uniform distribution OR huge right tail
2. **Log histogram**: Multiple distinct peaks (bimodal)
3. **Pie chart**: >1% red (timeouts)
4. **Time series**: Upward trend OR clustered timeouts
5. **Scatter**: Many red dots OR huge gap between normal and outliers
6. **Box plot**: Huge boxes OR very long whiskers OR many outlier dots
7. **Percentile**: P99 > 5× P50

---

## Example Workflow

### First-time analysis:
```bash
# Generate full report
python cli.py report benchmark.csv -o reports/initial/

# Open the directory and look at the images
# Start with: status_breakdown.png (any timeouts?)
# Then: time_distribution.png (shape of distribution?)
# Then: scatter_outliers.png (how many outliers?)
```

### Comparing sizes:
```bash
# Generate with comparisons
python cli.py report benchmark_30.csv -o reports/comparison/ \
  -c benchmark_10.csv -c benchmark_20.csv

# Focus on: box_comparison.png and percentile_comparison.png
# Question: Does variability grow with size?
```

### Investigating timeouts:
```bash
# If status_breakdown shows timeouts
python cli.py timeouts benchmark.csv --details --export timeouts.csv

# Then look at: time_series.png
# Question: Are timeouts clustered or random?
```

---

## Tips for Reading the Charts

1. **Always start with the pie chart** - gives you the big picture
2. **Histogram tells you "typical"** - where most runs land
3. **Scatter tells you "outliers"** - the exceptional cases
4. **Box plot tells you "spread"** - how consistent performance is
5. **Time series tells you "stability"** - does performance drift over time?

6. **Compare median to mean**:
   - Close together → symmetric, normal distribution
   - Mean >> median → heavy right tail (outliers pulling average up)

7. **Check CV (coefficient of variation)**:
   - <20% → Good consistency
   - >50% → Performance is unpredictable

8. **Look at IQR (the box size)**:
   - Small box → tight clustering, predictable
   - Large box → spread out, variable

9. **Watch the P99**:
   - P99 close to median → tight distribution
   - P99 far from median → long tail, bad user experience

---

## File Locations

After running:
```bash
python cli.py report file.csv -o my_report/
```

You'll find:
```
my_report/
├── time_distribution.png        # Main histogram
├── time_distribution_log.png    # Log scale histogram
├── status_breakdown.png         # Pie chart
├── time_series.png              # Time over iterations
├── scatter_outliers.png         # Outlier visualization
├── box_comparison.png           # Multi-file comparison
├── percentile_comparison.png    # Percentile bars
├── statistics.txt               # Text summary
├── statistics.json              # JSON data
└── timeouts.csv                 # Timeout cases (if any)
```

All PNG files are 300 DPI, suitable for presentations and publications.
